{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7ebd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import getpass\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084054c8",
   "metadata": {},
   "source": [
    "# SETTING DRIVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76deb6a",
   "metadata": {},
   "source": [
    "```python\n",
    "def driver():\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "    s = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return webdriver.Chrome(service=s, options=options)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a56d5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"9ab5abd88d0c416d9b995d83ccf89991\", element=\"FC4A1FF7AC33FE6C584BA7F67175CDF2_element_19\")>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Configurar el WebDriver para Google Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "# Abrir una página web\n",
    "url ='https://www.linkedin.com/jobs/search/?currentJobId=3622216694&geoId=107025191&keywords=data&location=Barcelona%2C%20Catalonia%2C%20Spain&refresh=true'\n",
    "\n",
    "wd.get(url)\n",
    "\n",
    "no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))\n",
    "\n",
    "wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa5e071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144dd0c",
   "metadata": {},
   "source": [
    "# Browse all the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8ab77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1db4d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"9ab5abd88d0c416d9b995d83ccf89991\", element=\"A06E6F6D131A61B7216440ACD838E408_element_36\")>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b479fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0180f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enconding_url (job_title, location, base_url):\n",
    "    \n",
    "    base_url = \"https://www.linkedin.com/jobs/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"keywords\": job_title,\n",
    "        \"location\": location,\n",
    "    }\n",
    "\n",
    "    encoded_params = \"&\" + \"&\".join([f\"{quote(k)}={quote(v)}\" for k, v in params.items()])\n",
    "    url_with_params = f\"{base_url}{encoded_params}\"\n",
    "\n",
    "    return url_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "424e7ee8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_jobs:  3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tècnic/a Suport Direcció Ref. 107-2023</td>\n",
       "      <td>Hospital Sant Joan de Déu Barcelona</td>\n",
       "      <td>Esplugues de Llobregat</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/t%C3%A8cnic-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hubtype</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Visualization Analyst</td>\n",
       "      <td>Experfy</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 meses</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-visuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Coniq</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Assistant in the Department of Economics</td>\n",
       "      <td>IESE Business School - University of Navarra</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/research-ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Data Engineer / Data Steward</td>\n",
       "      <td>Fundació de Recerca Sant Joan de Déu</td>\n",
       "      <td>Esplugues de Llobregat</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Junior Data Protection Officer (DPO)</td>\n",
       "      <td>Neuroelectrics</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/junior-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Operations Manager Relocation to Chicago, US</td>\n",
       "      <td>AIRE Ancient Baths</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/operations-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Developmental Tumor Research Area - Scientific...</td>\n",
       "      <td>Hospital Sant Joan de Déu Barcelona</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/developmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Winning Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0               Tècnic/a Suport Direcció Ref. 107-2023   \n",
       "1                                       Data Scientist   \n",
       "2                           Data Visualization Analyst   \n",
       "3                                         Data Analyst   \n",
       "4    Research Assistant in the Department of Economics   \n",
       "..                                                 ...   \n",
       "170                       Data Engineer / Data Steward   \n",
       "171               Junior Data Protection Officer (DPO)   \n",
       "172       Operations Manager Relocation to Chicago, US   \n",
       "173  Developmental Tumor Research Area - Scientific...   \n",
       "174                                       Data Analyst   \n",
       "\n",
       "                                          company                location  \\\n",
       "0             Hospital Sant Joan de Déu Barcelona  Esplugues de Llobregat   \n",
       "1                                         Hubtype               Barcelona   \n",
       "2                                         Experfy               Barcelona   \n",
       "3                                           Coniq               Barcelona   \n",
       "4    IESE Business School - University of Navarra               Barcelona   \n",
       "..                                            ...                     ...   \n",
       "170          Fundació de Recerca Sant Joan de Déu  Esplugues de Llobregat   \n",
       "171                                Neuroelectrics               Barcelona   \n",
       "172                            AIRE Ancient Baths               Barcelona   \n",
       "173           Hospital Sant Joan de Déu Barcelona               Barcelona   \n",
       "174                                 Winning Spain               Barcelona   \n",
       "\n",
       "               time                                                url  \n",
       "0     Hace 1 semana  https://es.linkedin.com/jobs/view/t%C3%A8cnic-...  \n",
       "1        Hace 1 mes  https://es.linkedin.com/jobs/view/data-scienti...  \n",
       "2      Hace 3 meses  https://es.linkedin.com/jobs/view/data-visuali...  \n",
       "3        Hace 1 mes  https://es.linkedin.com/jobs/view/data-analyst...  \n",
       "4     Hace 1 semana  https://es.linkedin.com/jobs/view/research-ass...  \n",
       "..              ...                                                ...  \n",
       "170      Hace 1 mes  https://es.linkedin.com/jobs/view/data-enginee...  \n",
       "171      Hace 1 mes  https://es.linkedin.com/jobs/view/junior-data-...  \n",
       "172  Hace 3 semanas  https://es.linkedin.com/jobs/view/operations-m...  \n",
       "173  Hace 3 semanas  https://es.linkedin.com/jobs/view/developmenta...  \n",
       "174   Hace 1 semana  https://es.linkedin.com/jobs/view/data-analyst...  \n",
       "\n",
       "[175 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import quote\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "\n",
    "\n",
    "def scraping_cards_on_the_left (job_title=\"data\", location=\"Barcelona, Catalonia, Spain\"):\n",
    "\n",
    "    # 1. Initializing driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # 2. Building the string\n",
    "\n",
    "    url = 'https://www.linkedin.com/jobs/search/?currentJobId=3622216694&geoId=107025191&keywords=data&location=Barcelona%2C%20Catalonia%2C%20Spain&refresh=true'\n",
    "    \n",
    "    \n",
    "    #3 . Get the data\n",
    "    wd.get(url)\n",
    "    no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))\n",
    "    print(\"no_of_jobs: \", no_of_jobs)\n",
    "    wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')\n",
    "    \n",
    "    jobsitos = []\n",
    "    els_jobs = []\n",
    "\n",
    "    i = 79\n",
    "\n",
    "    while i <= int(no_of_jobs/25) + 1:\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        i = i + 1\n",
    "        try:\n",
    "            job_lists = wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')\n",
    "            listita = job_lists.get_attribute('outerHTML')\n",
    "            soup = BeautifulSoup(listita, \"html.parser\")\n",
    "            soup_2 = soup.find_all(\"ul\", {\"class\": \"jobs-search__results-list\"})\n",
    "\n",
    "            # 1. EXTRAER JOB A JOB\n",
    "            for pagina in soup_2:\n",
    "                jobsitos.append(pagina)\n",
    "            #print(\"Jobsitos: \", jobsitos)\n",
    "\n",
    "\n",
    "            # 2. EXTRAER LA INFO DE CADA JOB\n",
    "            for job in jobsitos:\n",
    "                job_list = job.find_all(\"li\")\n",
    "\n",
    "                for the_one_job in job_list:\n",
    "                    url = the_one_job.find_all(\"a\")[0].get(\"href\")\n",
    "                    title = the_one_job.find_all(\"span\", {\"class\":\"sr-only\"})[0].text.strip()\n",
    "                    location = the_one_job.find_all(\"span\", {\"class\": \"job-search-card__location\" })[0].text.strip()\n",
    "                    company = the_one_job.find_all(\"h4\")[0].find(\"a\").text.strip()\n",
    "                    the_time = the_one_job.find_all(\"time\")[0].text.strip()\n",
    "\n",
    "\n",
    "                    dict_ = {\n",
    "                        \"title\": title,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"time\": the_time,\n",
    "                        \"url\": url,\n",
    "\n",
    "                    }\n",
    "                    \n",
    "                    if dict_ not in els_jobs:\n",
    "                        els_jobs.append(dict_)\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(5)\n",
    "\n",
    "    os.system(\"say -v Monica don escreipin\")\n",
    "    return pd.DataFrame(els_jobs)\n",
    "\n",
    "df_2 = scraping_cards_on_the_left ()\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465aca17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data_analysts_jobs175.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_csv(\"primer_exito_bro_to_wapo.csv\")\n",
    "#df_loaded.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6acd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_csv('all_data_analysts_jobs175.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded.url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc36fdf",
   "metadata": {},
   "source": [
    "# Scrapear detalles del job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c5e31",
   "metadata": {},
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "wd = webdriver.Chrome(options=options)\n",
    "url_job = \"https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card\"\n",
    "wd.get(url_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e5211",
   "metadata": {},
   "source": [
    "#Description’: jd\n",
    "#Seniority\n",
    "#Type’: emp_type,\n",
    "#Function’: job_func,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5336c",
   "metadata": {},
   "source": [
    "def logging_in (url):\n",
    "    \n",
    "    # 1. Initializing driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    wd.get(url)\n",
    "    \n",
    "    # 2. Try to log-in\n",
    "    \n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        driver.find_element(By.XPATH,'/html/body/header/nav/div/a[2]').click()\n",
    "        mail = 'cocazapata.21@gmail.com'\n",
    "        pw = 'fcbarcelona21'\n",
    "        time.sleep(3)\n",
    "        print(\"Logging in\")\n",
    "\n",
    "        username = wait.until(EC.visibility_of_element_located((By.ID, 'username'))).send_keys(mail)\n",
    "        password = wait.until(EC.visibility_of_element_located((By.ID, 'password'))).send_keys(pw)\n",
    "        password.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        return wd.page_source\n",
    "        \n",
    "    # 3. Otherwise: try to just get the content\n",
    "        \n",
    "    except:\n",
    "        print(\"Not logging in\")\n",
    "        time.sleep(2)\n",
    "        return wd.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cd438",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def info_un_job(url):\n",
    "    \n",
    "    response = logging_in (url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    descripcion = soup.find('div', {\"class\":'description__text description__text--rich'}).get_text(strip=True)\n",
    "    nivel_antiguedad = soup.find('span', {\"class\":\"description__job-criteria-text description__job-criteria-text--criteria\"}).get_text(strip=True)\n",
    "    sectores = soup.find('span', {\"class\":\"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
    "    \n",
    "    dict_ = {\n",
    "        \"description\": descripcion,\n",
    "        \"nivel_antiguedad\": nivel_antiguedad,\n",
    "        \"sectores\": sectores\n",
    "    }\n",
    "    \n",
    "    return dict_\n",
    "    \n",
    "\n",
    "url = 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'\n",
    "\n",
    "resultado = info_un_job(url)\n",
    "resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c41a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "984eea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>position</th>\n",
       "      <th>location</th>\n",
       "      <th>workplace</th>\n",
       "      <th>description</th>\n",
       "      <th>type_and_level</th>\n",
       "      <th>date</th>\n",
       "      <th>today</th>\n",
       "      <th>applicants</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coniq</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Barcelona, Catalonia, Spain</td>\n",
       "      <td>On-site</td>\n",
       "      <td>About the jobConiq is a dynamic, high growth U...</td>\n",
       "      <td>Full-time · Associate</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>11-07-2023</td>\n",
       "      <td>88 applicants</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company      position                     location workplace  \\\n",
       "0   Coniq  Data Analyst  Barcelona, Catalonia, Spain   On-site   \n",
       "\n",
       "                                         description         type_and_level  \\\n",
       "0  About the jobConiq is a dynamic, high growth U...  Full-time · Associate   \n",
       "\n",
       "          date       today     applicants  \\\n",
       "0  1 month ago  11-07-2023  88 applicants   \n",
       "\n",
       "                                                 url  \n",
       "0  https://es.linkedin.com/jobs/view/data-analyst...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def info_un_job(url):\n",
    "    wd = logging_in(url)\n",
    "    \n",
    "    response = wd.page_source\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    company = soup.find('span', {\"class\":\"jobs-unified-top-card__company-name\"}).get_text(strip=True)\n",
    "    position = soup.find('h1', {\"class\":\"t-24 t-bold jobs-unified-top-card__job-title\"}).get_text(strip=True)\n",
    "    location = soup.find('span', {\"class\":\"jobs-unified-top-card__bullet\"}).get_text(strip=True)\n",
    "    workplace = soup.find('span', {\"class\":\"jobs-unified-top-card__workplace-type\"}).get_text(strip=True)\n",
    "    type_and_level = soup.find(\"li\", {\"class\": \"jobs-unified-top-card__job-insight\"}).find(\"span\").text.strip()\n",
    "    description = soup.find('div', {\"class\": 'jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch'}).get_text(strip=True)\n",
    "    date = soup.find('span', {\"class\":\"jobs-unified-top-card__posted-date\"}).get_text(strip=True)\n",
    "    today = datetime.today().strftime('%d-%m-%Y')\n",
    "    link_text = soup.find('a', class_='app-aware-link').text.strip()\n",
    "    #workers = soup.find(\"li\", {\"class\": \"jobs-unified-top-card__job-insight\"}).find(\"span\").text.strip()\n",
    "    #num_alumnis = link_text.split()[0]\n",
    " \n",
    "     \n",
    "    # Handling missing elements\n",
    "    applicants = soup.find('span', class_='jobs-unified-top-card__applicant-count')\n",
    "    applicants = applicants.text.strip() if applicants else None\n",
    "    \n",
    "    dict_2 = {\n",
    "        \"company\" : company,\n",
    "        \"position\": position,\n",
    "        \"location\": location,\n",
    "        \"workplace\" : workplace,\n",
    "        \"description\": description,\n",
    "        \"type_and_level\": type_and_level,\n",
    "        \"date\": date,\n",
    "        \"today\": today,\n",
    "        #\"num_alumnis\": num_alumnis,\n",
    "        \"applicants\": applicants,\n",
    "        #\"workers\": workers,\n",
    "        \"url\": url,\n",
    "    }\n",
    "    \n",
    "    os.system(\"say -v Monica don escreipin\")\n",
    "\n",
    "    return dict_2\n",
    "\n",
    "url = 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'\n",
    "resultado = info_un_job(url)\n",
    "df_test = pd.DataFrame([resultado])\n",
    "df_test\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "beb59394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'Coniq',\n",
       " 'position': 'Data Analyst',\n",
       " 'location': 'Barcelona, Catalonia, Spain',\n",
       " 'workplace': 'On-site',\n",
       " 'description': 'About the jobConiq is a dynamic, high growth UK-based SaaS company that provides total customer engagement and loyalty services to many of the world’s leading property developers, shopping centers and retailers. Coniq powers well over £1 billion of sales annually for its clients, with more than 20 million consumers shopping at 1,800 brands in 24 countries worldwide, and has offices in Europe, the US and the Middle East.Coniq is looking for a Data Analst to join our brilliant team in Barcelona. We have a robust data infrastructure and we need someone to help us develop our industry-leading product offerings and capabilities to the next level. If you’re looking for a position which will offer variety, complexity, responsibility and challenges, this could be the role for you and we’d love to start a conversation!ResponsibilitiesDevelop the roadmap for our client facing reporting and dashboardsInteract with the internal client success teamCommunicate with clients to understand complex requirementsInterpret data, analyze results using statistical techniques and provide ongoing reportsAcquire data from primary or secondary data sources and maintain databases/data systemsIdentify, analyze, and interpret trends or patterns in complex data setsFilter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problemsWork with management to prioritize business and information needsRequirementsYou should be comfortable working in a multi-disciplinary, agile team. You are used to designing reports, analysing data, developing insights, implementing best practice and participating in group design sessions. You have experience of designing and implementing reporting dashboards and are able to pick up on key discussion points for communication with stakeholders. You can operate as a data and reporting expert with internal and external stakeholders.These are the skills you will be able to bring to the Coniq team:2-5 years’ experience as a data analystAt least 2 years experience with SQL and relational databases (MySQL knowledge a bonus)Experience with a data visualisation tool a mustComfortable working with data across a range of sources, shapes and sizes, and you are confident turning this into quality informationInterest in engaging with colleagues to define reporting requirements from clients and interallyInterest in proactively looking at data to pull out insights which can be fed back to clients or internallyAbility to stay on top of current business and industry trends around data prep and visualisation technologiesEffectively use Tableau and related technologies to deliver quality analytics and data insight to our clientsStrong communicator, with the ability to work across the different business teams to understand requirements and deliverPython knowledge a bonusExperience with Tableau a bonusExperience with Sisense a bonusInterest in growing into a team management roleRelevant university degree a bonusBenefitsWe offer a generous package, including:Competitive salaryCompany stock options25 days holiday plus statutory holidaysA day off to celebrate your birthdayA day off for your wellbeingShorter working hours on FridaysA strong company values framework, including paid leave for volunteering with approved charitiesRegular team building activitiesTraining & development allowanceNew employee referral schemeThis is a unique opportunity to join a VC-funded high growth SaaS business where we all share a passion to work together to build a great product and a great company. We are proud of our company culture and invest a great deal into making sure that we promote Diversity in the workplace. Together we come from over 20 nationalities and as a tech business, we are very proud of 50/50 gender split.',\n",
       " 'type_and_level': 'Full-time · Associate',\n",
       " 'date': '1 month ago',\n",
       " 'today': '11-07-2023',\n",
       " 'applicants': '88 applicants',\n",
       " 'url': 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dd127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c67cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultado = info_un_job(\"https://www.linkedin.com/jobs/view/3647687937/?alternateChannel=search&refId=zJ444F2EhVjX43eyXSKCCg%3D%3D&trackingId=isOlTSiDs2x5SVr3SQs6HQ%3D%3D\", wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb57ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851b228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ac35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070e997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335bf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Assuming \"all_data_analysts_jobs\" is the dataframe containing the URLs\n",
    "all_data_analysts_jobs =pd.read_csv('all_data_analysts_jobs.csv') # Replace ... with your actual dataframe creation code\n",
    "\n",
    "# Create an empty dataframe to store the extracted information\n",
    "extracted_data = pd.DataFrame(columns=[\"company\", \"position\", \"location\", \"workplace\", \"description\", \"type_and_level\", \"date\"])\n",
    "\n",
    "# Iterate over each URL in the dataframe\n",
    "for index, row in all_data_analysts_jobs.iterrows():\n",
    "    url = row['url']\n",
    "    wd = logging_in(url)\n",
    "    resultado = info_un_job(url, wd)\n",
    "    \n",
    "    # Append the extracted information to the dataframe\n",
    "    extracted_data = extracted_data.append(resultado, ignore_index=True)\n",
    "    \n",
    "    # Add a delay of 5 seconds between each iteration\n",
    "    time.sleep(5)\n",
    "\n",
    "# Save the extracted data to a new dataframe or a file\n",
    "extracted_data.to_csv(\"extracted_data.csv\", index=False)  # Example: Save as CSV file\n",
    "# extracted_data.to_excel(\"extracted_data.xlsx\", index=False)  # Example: Save as Excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aa1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc4138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfabb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7e945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82826992",
   "metadata": {},
   "source": [
    "jobsasos = []\n",
    "\n",
    "for item in range(len(jobs)):\n",
    "\n",
    "    # Haciendo clic en el trabajo para ver los detalles del trabajo\n",
    "    job_click_path = f'/html/body/main/div/section[2]/ul/li[{item+1}]/img'\n",
    "    job_click = jobs[item].find_element_by_xpath(job_click_path).click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    jd_path = '/html/body/main/section/div[2]/section[2]/div'\n",
    "    jd = wd.find_element_by_xpath(jd_path).get_attribute('innerText')\n",
    "    jd.append(jd)\n",
    "\n",
    "    seniority_path = '/html/body/main/section/div[2]/section[2]/ul/li[1]/span'\n",
    "    seniority = wd.find_element_by_xpath(seniority_path).get_attribute('innerText')\n",
    "    seniority.append(seniority)\n",
    "\n",
    "    emp_type_path = '/html/body/main/section/div[2]/section[2]/ul/li[2]/span'\n",
    "    emp_type = wd.find_element_by_xpath(emp_type_path).get_attribute('innerText')\n",
    "    emp_type.append(emp_type)\n",
    "\n",
    "    job_func_path = '/html/body/main/section/div[2]/section[2]/ul/li[3]/span'\n",
    "    job_func_elements = wd.find_elements_by_xpath(job_func_path)\n",
    "    for element in job_func_elements:\n",
    "        job_func.append(element.get_attribute('innerText'))\n",
    "    job_func_final = ', '.join(job_func)\n",
    "    job_func.append(job_func_final)\n",
    "\n",
    "    industries_path = '/html/body/main/section/div[2]/section[2]/ul/li[4]/span'\n",
    "    industries_elements = wd.find_elements_by_xpath(industries_path)\n",
    "    for element in industries_elements:\n",
    "        industries.append(element.get_attribute('innerText'))\n",
    "    industries_final = ', '.join(industries)\n",
    "    #industries.append(industries_final)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_data = pd.DataFrame({\n",
    "    'ID': job_id,\n",
    "    'Date': date,\n",
    "    'Company': company_name,\n",
    "    'Title': job_title,\n",
    "    'Location': location,\n",
    "    'Description': jd,\n",
    "    'Level': seniority,\n",
    "    'Type': emp_type,\n",
    "    'Function': job_func,\n",
    "    'Industry': industries,\n",
    "    'Link': job_link\n",
    "})\n",
    "\n",
    "# Limpiar la columna \"Description\"\n",
    "job_data['Description'] = job_data['Description'].str.replace('\\n', ' ')\n",
    "\n",
    "job_data.to_excel('LinkedIn Job Data_Data Scientist.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame({‘ID’: job_id,\n",
    "‘Date’: date,\n",
    "‘Company’: company_name,\n",
    "‘Title’: job_title,\n",
    "‘Location’: location,\n",
    "'Description’: jd,\n",
    "‘Level’: seniority,\n",
    "‘Type’: emp_type,\n",
    "‘Function’: job_func,\n",
    "‘Industry’: industries,\n",
    "‘Link’: job_link\n",
    "})\n",
    "# cleaning description column\n",
    "job_data[‘Description’] = job_data[‘Description’].str.replace(‘\\n’,’ ‘)\n",
    "job_data.to_excel('LinkedIn Job Data_Data Scientist.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecde766",
   "metadata": {},
   "source": [
    "```python\n",
    "# Data management\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Databases\n",
    "import sqlalchemy as alch\n",
    "from getpass import getpass\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Languages\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "#import es_core_news_sm\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"I love writing code in Python. I love Python code\",\n",
    "        \"I hate writing code in Java. I hate Java code\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_matrix = cv.fit_transform(df['text'])\n",
    "df_dtm = pd.DataFrame(cv_matrix.toarray(),\n",
    "                      index=df['review'].values,\n",
    "                      columns=cv.get_feature_names())\n",
    "df_dtm\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
