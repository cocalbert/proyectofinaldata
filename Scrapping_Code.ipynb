{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7ebd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import getpass\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084054c8",
   "metadata": {},
   "source": [
    "# SETTING DRIVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76deb6a",
   "metadata": {},
   "source": [
    "```python\n",
    "def driver():\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "    s = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return webdriver.Chrome(service=s, options=options)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a56d5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"0f0c817639baa1603220c339af148161\", element=\"7FF4C1D2CE68F7CC5EAD4B10EB906465_element_19\")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Configurar el WebDriver para Google Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "# Abrir una página web\n",
    "url ='https://www.linkedin.com/jobs/search/?currentJobId=3652284026&distance=25&geoId=107025191&keywords=data%20analyst&refresh=true'\n",
    "\n",
    "wd.get(url)\n",
    "\n",
    "no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))\n",
    "\n",
    "wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa5e071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144dd0c",
   "metadata": {},
   "source": [
    "# Browse all the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8ab77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db4d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"0f0c817639baa1603220c339af148161\", element=\"5DF6B02FF10E9E1F02D20A14D5FB89C3_element_33\")>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b479fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0180f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enconding_url (job_title, location, base_url):\n",
    "    \n",
    "    base_url = \"https://www.linkedin.com/jobs/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"keywords\": job_title,\n",
    "        \"location\": location,\n",
    "    }\n",
    "\n",
    "    encoded_params = \"&\" + \"&\".join([f\"{quote(k)}={quote(v)}\" for k, v in params.items()])\n",
    "    url_with_params = f\"{base_url}{encoded_params}\"\n",
    "\n",
    "    return url_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "424e7ee8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_jobs:  2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 2 meses</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Analytics Engineer</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/junior-analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>HP</td>\n",
       "      <td>Sant Cugat del Vallès</td>\n",
       "      <td>Hace 7 horas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analytics Engineer</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 meses</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/analytics-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist (Andorra)</td>\n",
       "      <td>Gauss &amp; Neumann</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Junior Engineer</td>\n",
       "      <td>Submer</td>\n",
       "      <td>Rubí</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/junior-engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Big Data Software Engineer</td>\n",
       "      <td>Bridgestone Mobility Solutions</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/big-data-sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>Data Engineer Senior</td>\n",
       "      <td>KPMG España</td>\n",
       "      <td>Hospitalet de Llobregat</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>STAGER DATA SCIENTIST</td>\n",
       "      <td>MANGO</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/stager-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>INTEGRATED BUSINESS PLANNING ANALYST 1</td>\n",
       "      <td>Grifols</td>\n",
       "      <td>Sant Cugat del Vallès</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/integrated-b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title                         company  \\\n",
       "0                              Data Analyst                        Exoticca   \n",
       "1                 Junior Analytics Engineer                        Exoticca   \n",
       "2                              Data Analyst                              HP   \n",
       "3                        Analytics Engineer                        Exoticca   \n",
       "4                  Data Scientist (Andorra)                 Gauss & Neumann   \n",
       "..                                      ...                             ...   \n",
       "382                         Junior Engineer                          Submer   \n",
       "383              Big Data Software Engineer  Bridgestone Mobility Solutions   \n",
       "384                    Data Engineer Senior                     KPMG España   \n",
       "385                   STAGER DATA SCIENTIST                           MANGO   \n",
       "386  INTEGRATED BUSINESS PLANNING ANALYST 1                         Grifols   \n",
       "\n",
       "                    location            time  \\\n",
       "0                  Barcelona    Hace 2 meses   \n",
       "1                  Barcelona  Hace 3 semanas   \n",
       "2      Sant Cugat del Vallès    Hace 7 horas   \n",
       "3                  Barcelona    Hace 3 meses   \n",
       "4                  Barcelona      Hace 1 mes   \n",
       "..                       ...             ...   \n",
       "382                     Rubí   Hace 1 semana   \n",
       "383                Barcelona  Hace 3 semanas   \n",
       "384  Hospitalet de Llobregat   Hace 1 semana   \n",
       "385                Barcelona  Hace 3 semanas   \n",
       "386    Sant Cugat del Vallès  Hace 3 semanas   \n",
       "\n",
       "                                                   url  \n",
       "0    https://es.linkedin.com/jobs/view/data-analyst...  \n",
       "1    https://es.linkedin.com/jobs/view/junior-analy...  \n",
       "2    https://es.linkedin.com/jobs/view/data-analyst...  \n",
       "3    https://es.linkedin.com/jobs/view/analytics-en...  \n",
       "4    https://es.linkedin.com/jobs/view/data-scienti...  \n",
       "..                                                 ...  \n",
       "382  https://es.linkedin.com/jobs/view/junior-engin...  \n",
       "383  https://es.linkedin.com/jobs/view/big-data-sof...  \n",
       "384  https://es.linkedin.com/jobs/view/data-enginee...  \n",
       "385  https://es.linkedin.com/jobs/view/stager-data-...  \n",
       "386  https://es.linkedin.com/jobs/view/integrated-b...  \n",
       "\n",
       "[387 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import quote\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "\n",
    "def scraping_cards_on_the_left(job_title=\"data\", location=\"Barcelona, Catalonia, Spain\"):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    url = 'https://www.linkedin.com/jobs/search/?currentJobId=3652284026&distance=25&geoId=107025191&keywords=data%20analyst&refresh=true'\n",
    "    \n",
    "    wd.get(url)\n",
    "    no_of_jobs = int(wd.find_element(By.CSS_SELECTOR, \"h1>span\").get_attribute('innerText'))\n",
    "    print(\"no_of_jobs: \", no_of_jobs)\n",
    "    wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')\n",
    "    \n",
    "    jobsitos = []\n",
    "    els_jobs = []\n",
    "    i = 1\n",
    "\n",
    "    previous_page_content = \"\"  # Variable para almacenar el contenido de la página anterior\n",
    "\n",
    "    while i <= int(no_of_jobs/25) + 1:\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        i = i + 1\n",
    "        try:\n",
    "            job_lists = wd.find_element(By.CSS_SELECTOR, '.jobs-search__results-list')\n",
    "            listita = job_lists.get_attribute('outerHTML')\n",
    "            soup = BeautifulSoup(listita, \"html.parser\")\n",
    "            soup_2 = soup.find_all(\"ul\", {\"class\": \"jobs-search__results-list\"})\n",
    "\n",
    "            # 1. EXTRAER JOB A JOB\n",
    "            for pagina in soup_2:\n",
    "                jobsitos.append(pagina)\n",
    "\n",
    "            # 2. EXTRAER LA INFO DE CADA JOB\n",
    "            for job in jobsitos:\n",
    "                job_list = job.find_all(\"li\")\n",
    "\n",
    "                for the_one_job in job_list:\n",
    "                    url = the_one_job.find_all(\"a\")[0].get(\"href\")\n",
    "                    title = the_one_job.find_all(\"span\", {\"class\":\"sr-only\"})[0].text.strip()\n",
    "                    location = the_one_job.find_all(\"span\", {\"class\": \"job-search-card__location\" })[0].text.strip()\n",
    "                    company = the_one_job.find_all(\"h4\")[0].find(\"a\").text.strip()\n",
    "                    the_time = the_one_job.find_all(\"time\")[0].text.strip()\n",
    "\n",
    "                    dict_ = {\n",
    "                        \"title\": title,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"time\": the_time,\n",
    "                        \"url\": url,\n",
    "                    }\n",
    "\n",
    "                    if dict_ not in els_jobs:\n",
    "                        els_jobs.append(dict_)\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Verificar si el botón \"Ver más empleos\" está presente y hacer clic en él\n",
    "            show_more_button = wd.find_elements(By.CSS_SELECTOR, '.infinite-scroller__show-more-button')\n",
    "            if show_more_button:\n",
    "                show_more_button[0].click()\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # Verificar si el contenido de la página ha cambiado después de hacer clic\n",
    "                current_page_content = wd.page_source\n",
    "\n",
    "                if current_page_content == previous_page_content:\n",
    "                    break  # Salir del bucle si no hay más trabajos disponibles\n",
    "\n",
    "                previous_page_content = current_page_content\n",
    "\n",
    "            else:\n",
    "                break  # Salir del bucle si el botón no está presente\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(5)\n",
    "\n",
    "    os.system(\"say -v Monica don escreipin\")\n",
    "    return pd.DataFrame(els_jobs)\n",
    "\n",
    "df_2 = scraping_cards_on_the_left()\n",
    "df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "465aca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('ultimo_funcion_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5977689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last = pd.read_csv('ultimo_funcion_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34a1187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_primerexito = pd.read_csv('primer_exito_bro_to_wapo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24b30755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medio = pd.read_csv('all_data_analysts_jobs175.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e857b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_primero = pd.read_csv('all_data_analysts_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data_analysts_jobs175.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d06d581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_last, df_primero, df_medio, df_primerexito]).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d89bdf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('first_function_datajobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81befb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 2 meses</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Analytics Engineer</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/junior-analy...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>HP</td>\n",
       "      <td>Sant Cugat del Vallès</td>\n",
       "      <td>Hace 7 horas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analytics Engineer</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 meses</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/analytics-en...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist (Andorra)</td>\n",
       "      <td>Gauss &amp; Neumann</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-scienti...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Exoticca</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 3 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-enginee...</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>European Data Analyst</td>\n",
       "      <td>Diversey</td>\n",
       "      <td>Viladecans</td>\n",
       "      <td>Hace 1 mes</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/european-dat...</td>\n",
       "      <td>246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Zurich Insurance</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 2 semanas</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/senior-data-...</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Power BI Consultant (I&amp;D)</td>\n",
       "      <td>Capgemini</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 1 semana</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/power-bi-con...</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Data Analyst (Hybrid, Barcelona based candidat...</td>\n",
       "      <td>Joppy</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Hace 5 días</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>912 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title           company  \\\n",
       "0                                         Data Analyst          Exoticca   \n",
       "1                            Junior Analytics Engineer          Exoticca   \n",
       "2                                         Data Analyst                HP   \n",
       "3                                   Analytics Engineer          Exoticca   \n",
       "4                             Data Scientist (Andorra)   Gauss & Neumann   \n",
       "..                                                 ...               ...   \n",
       "245                                      Data Engineer          Exoticca   \n",
       "246                              European Data Analyst          Diversey   \n",
       "247                                Senior Data Analyst  Zurich Insurance   \n",
       "248                          Power BI Consultant (I&D)         Capgemini   \n",
       "249  Data Analyst (Hybrid, Barcelona based candidat...             Joppy   \n",
       "\n",
       "                  location            time  \\\n",
       "0                Barcelona    Hace 2 meses   \n",
       "1                Barcelona  Hace 3 semanas   \n",
       "2    Sant Cugat del Vallès    Hace 7 horas   \n",
       "3                Barcelona    Hace 3 meses   \n",
       "4                Barcelona      Hace 1 mes   \n",
       "..                     ...             ...   \n",
       "245              Barcelona  Hace 3 semanas   \n",
       "246             Viladecans      Hace 1 mes   \n",
       "247              Barcelona  Hace 2 semanas   \n",
       "248              Barcelona   Hace 1 semana   \n",
       "249              Barcelona     Hace 5 días   \n",
       "\n",
       "                                                   url  Unnamed: 0  \n",
       "0    https://es.linkedin.com/jobs/view/data-analyst...         NaN  \n",
       "1    https://es.linkedin.com/jobs/view/junior-analy...         NaN  \n",
       "2    https://es.linkedin.com/jobs/view/data-analyst...         NaN  \n",
       "3    https://es.linkedin.com/jobs/view/analytics-en...         NaN  \n",
       "4    https://es.linkedin.com/jobs/view/data-scienti...         NaN  \n",
       "..                                                 ...         ...  \n",
       "245  https://es.linkedin.com/jobs/view/data-enginee...       245.0  \n",
       "246  https://es.linkedin.com/jobs/view/european-dat...       246.0  \n",
       "247  https://es.linkedin.com/jobs/view/senior-data-...       247.0  \n",
       "248  https://es.linkedin.com/jobs/view/power-bi-con...       248.0  \n",
       "249  https://es.linkedin.com/jobs/view/data-analyst...       249.0  \n",
       "\n",
       "[912 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54ce6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_2 = df_combined.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df6acd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_2.to_csv('csv_urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c09204fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://es.linkedin.com/jobs/view/data-analyst-at-exoticca-3622172076?refId=jWf2Fweu6LwaRUAHa5CNVQ%3D%3D&trackingId=vzc7hXRuY8DFO0xPl1w3VA%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_2.url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc36fdf",
   "metadata": {},
   "source": [
    "# Scrapear detalles del job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c5e31",
   "metadata": {},
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "wd = webdriver.Chrome(options=options)\n",
    "url_job = \"https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card\"\n",
    "wd.get(url_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e5211",
   "metadata": {},
   "source": [
    "#Description’: jd\n",
    "#Seniority\n",
    "#Type’: emp_type,\n",
    "#Function’: job_func,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5336c",
   "metadata": {},
   "source": [
    "def logging_in (url):\n",
    "    \n",
    "    # 1. Initializing driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    wd.get(url)\n",
    "    \n",
    "    # 2. Try to log-in\n",
    "    \n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        driver.find_element(By.XPATH,'/html/body/header/nav/div/a[2]').click()\n",
    "        mail = 'cocazapata.21@gmail.com'\n",
    "        pw = 'fcbarcelona21'\n",
    "        time.sleep(3)\n",
    "        print(\"Logging in\")\n",
    "\n",
    "        username = wait.until(EC.visibility_of_element_located((By.ID, 'username'))).send_keys(mail)\n",
    "        password = wait.until(EC.visibility_of_element_located((By.ID, 'password'))).send_keys(pw)\n",
    "        password.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        return wd.page_source\n",
    "        \n",
    "    # 3. Otherwise: try to just get the content\n",
    "        \n",
    "    except:\n",
    "        print(\"Not logging in\")\n",
    "        time.sleep(2)\n",
    "        return wd.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cd438",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def info_un_job(url):\n",
    "    \n",
    "    response = logging_in (url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    descripcion = soup.find('div', {\"class\":'description__text description__text--rich'}).get_text(strip=True)\n",
    "    nivel_antiguedad = soup.find('span', {\"class\":\"description__job-criteria-text description__job-criteria-text--criteria\"}).get_text(strip=True)\n",
    "    sectores = soup.find('span', {\"class\":\"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
    "    \n",
    "    dict_ = {\n",
    "        \"description\": descripcion,\n",
    "        \"nivel_antiguedad\": nivel_antiguedad,\n",
    "        \"sectores\": sectores\n",
    "    }\n",
    "    \n",
    "    return dict_\n",
    "    \n",
    "\n",
    "url = 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'\n",
    "\n",
    "resultado = info_un_job(url)\n",
    "resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c41a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fdd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557fc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "984eea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>position</th>\n",
       "      <th>location</th>\n",
       "      <th>workplace</th>\n",
       "      <th>description</th>\n",
       "      <th>type_and_level</th>\n",
       "      <th>date</th>\n",
       "      <th>today</th>\n",
       "      <th>applicants</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coniq</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Barcelona, Catalonia, Spain</td>\n",
       "      <td>On-site</td>\n",
       "      <td>About the jobConiq is a dynamic, high growth U...</td>\n",
       "      <td>Full-time · Associate</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>11-07-2023</td>\n",
       "      <td>88 applicants</td>\n",
       "      <td>https://es.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company      position                     location workplace  \\\n",
       "0   Coniq  Data Analyst  Barcelona, Catalonia, Spain   On-site   \n",
       "\n",
       "                                         description         type_and_level  \\\n",
       "0  About the jobConiq is a dynamic, high growth U...  Full-time · Associate   \n",
       "\n",
       "          date       today     applicants  \\\n",
       "0  1 month ago  11-07-2023  88 applicants   \n",
       "\n",
       "                                                 url  \n",
       "0  https://es.linkedin.com/jobs/view/data-analyst...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def logging_in(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    wd.get(url)\n",
    "\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        wd.find_element(By.XPATH, '/html/body/header/nav/div/a[2]').click()\n",
    "        mail = 'cocazapata.21@gmail.com'\n",
    "        pw = 'fcbarcelona21'\n",
    "        time.sleep(3)\n",
    "        print(\"Logging in\")\n",
    "\n",
    "        username = wd.find_element(By.ID, 'username')\n",
    "        username.send_keys(mail)\n",
    "        password = wd.find_element(By.ID, 'password')\n",
    "        password.send_keys(pw)\n",
    "        password.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        return wd\n",
    "    except:\n",
    "        print(\"Not logging in\")\n",
    "        time.sleep(2)\n",
    "        return wd\n",
    "\n",
    "\n",
    "def info_un_job(url):\n",
    "    wd = logging_in(url)\n",
    "    \n",
    "    response = wd.page_source\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    company = soup.find('span', {\"class\":\"jobs-unified-top-card__company-name\"}).get_text(strip=True)\n",
    "    position = soup.find('h1', {\"class\":\"t-24 t-bold jobs-unified-top-card__job-title\"}).get_text(strip=True)\n",
    "    location = soup.find('span', {\"class\":\"jobs-unified-top-card__bullet\"}).get_text(strip=True)\n",
    "    workplace = soup.find('span', {\"class\":\"jobs-unified-top-card__workplace-type\"}).get_text(strip=True)\n",
    "    type_and_level = soup.find(\"li\", {\"class\": \"jobs-unified-top-card__job-insight\"}).find(\"span\").text.strip()\n",
    "    description = soup.find('div', {\"class\": 'jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch'}).get_text(strip=True)\n",
    "    date = soup.find('span', {\"class\":\"jobs-unified-top-card__posted-date\"}).get_text(strip=True)\n",
    "    today = datetime.today().strftime('%d-%m-%Y')\n",
    "    link_text = soup.find('a', class_='app-aware-link').text.strip()\n",
    "    #workers = soup.find(\"li\", {\"class\": \"jobs-unified-top-card__job-insight\"}).find(\"span\").text.strip()\n",
    "    #num_alumnis = link_text.split()[0]\n",
    " \n",
    "     \n",
    "    # Handling missing elements\n",
    "    applicants = soup.find('span', class_='jobs-unified-top-card__applicant-count')\n",
    "    applicants = applicants.text.strip() if applicants else None\n",
    "    \n",
    "    dict_2 = {\n",
    "        \"company\" : company,\n",
    "        \"position\": position,\n",
    "        \"location\": location,\n",
    "        \"workplace\" : workplace,\n",
    "        \"description\": description,\n",
    "        \"type_and_level\": type_and_level,\n",
    "        \"date\": date,\n",
    "        \"today\": today,\n",
    "        #\"num_alumnis\": num_alumnis,\n",
    "        \"applicants\": applicants,\n",
    "        #\"workers\": workers,\n",
    "        \"url\": url,\n",
    "    }\n",
    "    \n",
    "    os.system(\"say -v Monica don escreipin\")\n",
    "\n",
    "    return dict_2\n",
    "\n",
    "url = 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'\n",
    "resultado = info_un_job(url)\n",
    "df_test = pd.DataFrame([resultado])\n",
    "df_test\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beb59394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'Coniq',\n",
       " 'position': 'Data Analyst',\n",
       " 'location': 'Barcelona, Catalonia, Spain',\n",
       " 'workplace': 'On-site',\n",
       " 'description': 'About the jobConiq is a dynamic, high growth UK-based SaaS company that provides total customer engagement and loyalty services to many of the world’s leading property developers, shopping centers and retailers. Coniq powers well over £1 billion of sales annually for its clients, with more than 20 million consumers shopping at 1,800 brands in 24 countries worldwide, and has offices in Europe, the US and the Middle East.Coniq is looking for a Data Analst to join our brilliant team in Barcelona. We have a robust data infrastructure and we need someone to help us develop our industry-leading product offerings and capabilities to the next level. If you’re looking for a position which will offer variety, complexity, responsibility and challenges, this could be the role for you and we’d love to start a conversation!ResponsibilitiesDevelop the roadmap for our client facing reporting and dashboardsInteract with the internal client success teamCommunicate with clients to understand complex requirementsInterpret data, analyze results using statistical techniques and provide ongoing reportsAcquire data from primary or secondary data sources and maintain databases/data systemsIdentify, analyze, and interpret trends or patterns in complex data setsFilter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problemsWork with management to prioritize business and information needsRequirementsYou should be comfortable working in a multi-disciplinary, agile team. You are used to designing reports, analysing data, developing insights, implementing best practice and participating in group design sessions. You have experience of designing and implementing reporting dashboards and are able to pick up on key discussion points for communication with stakeholders. You can operate as a data and reporting expert with internal and external stakeholders.These are the skills you will be able to bring to the Coniq team:2-5 years’ experience as a data analystAt least 2 years experience with SQL and relational databases (MySQL knowledge a bonus)Experience with a data visualisation tool a mustComfortable working with data across a range of sources, shapes and sizes, and you are confident turning this into quality informationInterest in engaging with colleagues to define reporting requirements from clients and interallyInterest in proactively looking at data to pull out insights which can be fed back to clients or internallyAbility to stay on top of current business and industry trends around data prep and visualisation technologiesEffectively use Tableau and related technologies to deliver quality analytics and data insight to our clientsStrong communicator, with the ability to work across the different business teams to understand requirements and deliverPython knowledge a bonusExperience with Tableau a bonusExperience with Sisense a bonusInterest in growing into a team management roleRelevant university degree a bonusBenefitsWe offer a generous package, including:Competitive salaryCompany stock options25 days holiday plus statutory holidaysA day off to celebrate your birthdayA day off for your wellbeingShorter working hours on FridaysA strong company values framework, including paid leave for volunteering with approved charitiesRegular team building activitiesTraining & development allowanceNew employee referral schemeThis is a unique opportunity to join a VC-funded high growth SaaS business where we all share a passion to work together to build a great product and a great company. We are proud of our company culture and invest a great deal into making sure that we promote Diversity in the workplace. Together we come from over 20 nationalities and as a tech business, we are very proud of 50/50 gender split.',\n",
       " 'type_and_level': 'Full-time · Associate',\n",
       " 'date': '1 month ago',\n",
       " 'today': '11-07-2023',\n",
       " 'applicants': '88 applicants',\n",
       " 'url': 'https://es.linkedin.com/jobs/view/data-analyst-at-coniq-3652827423?refId=LmHLqc2XCC2YJGeJ%2BOJlfw%3D%3D&trackingId=CtHhJ%2Fe3ZQTxs8KZpryElw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dd127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n",
      "The version of chrome cannot be detected. Trying with latest driver version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in\n",
      "Logging in\n",
      "Logging in\n",
      "Logging in\n",
      "Logging in\n",
      "Logging in\n",
      "Logging in\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# Cargar el DataFrame con las URLs\n",
    "csv_urls = pd.read_csv('csv_urls.csv')\n",
    "\n",
    "# Función para procesar una URL y extraer la información de la vacante\n",
    "def procesar_url(url):\n",
    "    resultado = info_un_job(url)\n",
    "    return resultado\n",
    "\n",
    "# Especificar el número de hilos a utilizar\n",
    "num_hilos = 8\n",
    "\n",
    "# Crear un ThreadPoolExecutor con el número de hilos especificado\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_hilos)\n",
    "\n",
    "# Crear una lista para almacenar los resultados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre la columna \"url\" y enviar cada URL al ThreadPoolExecutor para procesarla en paralelo\n",
    "for url in csv_urls['url']:\n",
    "    future = executor.submit(procesar_url, url)\n",
    "    results.append(future)\n",
    "\n",
    "# Obtener los resultados a medida que estén listos\n",
    "df_results = pd.DataFrame([future.result() for future in concurrent.futures.as_completed(results)])\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb57ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851b228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ac35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070e997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335bf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Assuming \"all_data_analysts_jobs\" is the dataframe containing the URLs\n",
    "all_data_analysts_jobs =pd.read_csv('all_data_analysts_jobs.csv') # Replace ... with your actual dataframe creation code\n",
    "\n",
    "# Create an empty dataframe to store the extracted information\n",
    "extracted_data = pd.DataFrame(columns=[\"company\", \"position\", \"location\", \"workplace\", \"description\", \"type_and_level\", \"date\"])\n",
    "\n",
    "# Iterate over each URL in the dataframe\n",
    "for index, row in all_data_analysts_jobs.iterrows():\n",
    "    url = row['url']\n",
    "    wd = logging_in(url)\n",
    "    resultado = info_un_job(url, wd)\n",
    "    \n",
    "    # Append the extracted information to the dataframe\n",
    "    extracted_data = extracted_data.append(resultado, ignore_index=True)\n",
    "    \n",
    "    # Add a delay of 5 seconds between each iteration\n",
    "    time.sleep(5)\n",
    "\n",
    "# Save the extracted data to a new dataframe or a file\n",
    "extracted_data.to_csv(\"extracted_data.csv\", index=False)  # Example: Save as CSV file\n",
    "# extracted_data.to_excel(\"extracted_data.xlsx\", index=False)  # Example: Save as Excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aa1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc4138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfabb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7e945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82826992",
   "metadata": {},
   "source": [
    "jobsasos = []\n",
    "\n",
    "for item in range(len(jobs)):\n",
    "\n",
    "    # Haciendo clic en el trabajo para ver los detalles del trabajo\n",
    "    job_click_path = f'/html/body/main/div/section[2]/ul/li[{item+1}]/img'\n",
    "    job_click = jobs[item].find_element_by_xpath(job_click_path).click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    jd_path = '/html/body/main/section/div[2]/section[2]/div'\n",
    "    jd = wd.find_element_by_xpath(jd_path).get_attribute('innerText')\n",
    "    jd.append(jd)\n",
    "\n",
    "    seniority_path = '/html/body/main/section/div[2]/section[2]/ul/li[1]/span'\n",
    "    seniority = wd.find_element_by_xpath(seniority_path).get_attribute('innerText')\n",
    "    seniority.append(seniority)\n",
    "\n",
    "    emp_type_path = '/html/body/main/section/div[2]/section[2]/ul/li[2]/span'\n",
    "    emp_type = wd.find_element_by_xpath(emp_type_path).get_attribute('innerText')\n",
    "    emp_type.append(emp_type)\n",
    "\n",
    "    job_func_path = '/html/body/main/section/div[2]/section[2]/ul/li[3]/span'\n",
    "    job_func_elements = wd.find_elements_by_xpath(job_func_path)\n",
    "    for element in job_func_elements:\n",
    "        job_func.append(element.get_attribute('innerText'))\n",
    "    job_func_final = ', '.join(job_func)\n",
    "    job_func.append(job_func_final)\n",
    "\n",
    "    industries_path = '/html/body/main/section/div[2]/section[2]/ul/li[4]/span'\n",
    "    industries_elements = wd.find_elements_by_xpath(industries_path)\n",
    "    for element in industries_elements:\n",
    "        industries.append(element.get_attribute('innerText'))\n",
    "    industries_final = ', '.join(industries)\n",
    "    #industries.append(industries_final)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_data = pd.DataFrame({\n",
    "    'ID': job_id,\n",
    "    'Date': date,\n",
    "    'Company': company_name,\n",
    "    'Title': job_title,\n",
    "    'Location': location,\n",
    "    'Description': jd,\n",
    "    'Level': seniority,\n",
    "    'Type': emp_type,\n",
    "    'Function': job_func,\n",
    "    'Industry': industries,\n",
    "    'Link': job_link\n",
    "})\n",
    "\n",
    "# Limpiar la columna \"Description\"\n",
    "job_data['Description'] = job_data['Description'].str.replace('\\n', ' ')\n",
    "\n",
    "job_data.to_excel('LinkedIn Job Data_Data Scientist.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame({‘ID’: job_id,\n",
    "‘Date’: date,\n",
    "‘Company’: company_name,\n",
    "‘Title’: job_title,\n",
    "‘Location’: location,\n",
    "'Description’: jd,\n",
    "‘Level’: seniority,\n",
    "‘Type’: emp_type,\n",
    "‘Function’: job_func,\n",
    "‘Industry’: industries,\n",
    "‘Link’: job_link\n",
    "})\n",
    "# cleaning description column\n",
    "job_data[‘Description’] = job_data[‘Description’].str.replace(‘\\n’,’ ‘)\n",
    "job_data.to_excel('LinkedIn Job Data_Data Scientist.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecde766",
   "metadata": {},
   "source": [
    "```python\n",
    "# Data management\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Databases\n",
    "import sqlalchemy as alch\n",
    "from getpass import getpass\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Languages\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "#import es_core_news_sm\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"I love writing code in Python. I love Python code\",\n",
    "        \"I hate writing code in Java. I hate Java code\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_matrix = cv.fit_transform(df['text'])\n",
    "df_dtm = pd.DataFrame(cv_matrix.toarray(),\n",
    "                      index=df['review'].values,\n",
    "                      columns=cv.get_feature_names())\n",
    "df_dtm\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
